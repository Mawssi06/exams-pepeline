#######################################################################################################################


####Anlyses de nos données 


from pyspark.sql import SparkSession

# Créez une session Spark
spark = SparkSession.builder.appName("Suppression des doublons").getOrCreate()
# Chargez le fichier CSV dans un DataFrame
df_flight = spark.read.csv("/home/gomes/Documents/cours/conceptiin pleippleain/examen/archive/flights.csv" , header=True, inferSchema=True)

df_raw_flight = spark.read.csv("/home/gomes/Documents/cours/conceptiin pleippleain/examen/archive/raw-flight-data.csv" , header=True, inferSchema=True)

ecart =df_flight.subtract(df_raw_flight)
ecart.show()

#########################################################################################################################

## Intersection des deux tables flight et Raw_flight

from pyspark.sql import SparkSession

# Créez une session Spark
spark = SparkSession.builder.appName("Suppression des doublons").getOrCreate()

# Chargez le fichier CSV dans un DataFrame
df_flight = spark.read.csv("/home/gomes/Documents/cours/conceptiin pleippleain/examen/archive/flights.csv" , header=True, inferSchema=True)

df_raw_flight = spark.read.csv("/home/gomes/Documents/cours/conceptiin pleippleain/examen/archive/raw-flight-data.csv" , header=True, inferSchema=True)

intersection=df_raw_flight.intersect(df_flight)

#intersection.show(n=intersection.count(), truncate=False)

intersection.show()
print(intersection.count())


########################################################################################################################


###### Utilisation de linéaire regression

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import StringIndexer
from pyspark.sql.functions import col
from pyspark.sql.functions import when, col
from pyspark.sql.functions import mean


# 1. Créez une session Spark
spark = SparkSession.builder.appName("LinearRegressionExample").getOrCreate()
# 2. Chargez le fichier CSV dans un DataFrame
airports= spark.read.csv("/home/gomes/Documents/cours/conceptiin pleippleain/examen/archive/airports.csv",header=True, inferSchema=True)

df_flight = spark.read.csv("/home/gomes/Documents/cours/conceptiin pleippleain/examen/archive/flights.csv" , header=True, inferSchema=True)

df_raw_flight = spark.read.csv("/home/gomes/Documents/cours/conceptiin pleippleain/examen/archive/raw-flight-data.csv" , header=True, inferSchema=True)

dat=df_raw_flight.intersect(df_flight)


sans_doublons=dat.dropDuplicates()

df=sans_doublons.na.drop()

df_filtre = df.filter((df["DepDelay"].cast("int").isNotNull()) & (df["DepDelay"] >= 0) | (df["DepDelay"] < 0 ) &
                      (df["ArrDelay"].cast("int").isNotNull()) & (df["ArrDelay"] >= 0) |  (df["ArrDelay"] < 0))
    
stringIndexer = StringIndexer(inputCol="Carrier", outputCol="Carrier_to_numérique")

# Ajustez le modèle de conversion aux données
model = stringIndexer.fit(df_filtre )

# Transformez les données pour ajouter la colonne numérique
data = model.transform(df_filtre )

input_cols = [col for col in df.columns if col not in ['Carrier']]

# Créez le VectorAssembler
assembler = VectorAssembler(inputCols=input_cols, outputCol="features")

# Transformez le DataFrame en ajoutant la colonne "features"
data= assembler.transform(data)

train_ratio = 0.8  # 80% pour le jeu d'entraînement
test_ratio = 0.2   # 20% pour le jeu de test

# Utilisez la méthode randomSplit pour effectuer la division
train, test = data.randomSplit([train_ratio, test_ratio])

# Le DataFrame 'train' contient 80% des données
# Le DataFrame 'test' contient 20% des données

# Vous pouvez également afficher la taille de chaque jeu de données
print("Taille du jeu d'entraînement : ", train.count())
train.show()


#print("Taille du jeu de test : ", test.count())
#test.show ()
#lineaire regression pour le départ
lr = LinearRegression(featuresCol="features", labelCol="DepDelay")

# Utilisez la méthode .fit() pour ajuster le modèle aux données d'entraînement
model = lr.fit(train)

predictions_Dep = model.transform(test)

predictions_Dep = predictions_Dep.withColumnRenamed("prediction", "prediction_Depp")

predictions_Dep.show()

#lineaire regression pour l'arrivé
lr_2 = LinearRegression(featuresCol="features", labelCol="ArrDelay")

model = lr_2.fit(train)

predictions_Arr = model.transform(test)
predictions_Arr = predictions_Arr.withColumnRenamed("prediction", "prediction_Arri")
predictions_Arr.show()

# Utilisez RegressionEvaluator pour évaluer le modèle pour le départ
evaluator = RegressionEvaluator(labelCol="DepDelay", predictionCol="prediction_Depp", metricName="rmse")
rmse = evaluator.evaluate(predictions_Dep)

# Affichez la racine carrée de l'erreur quadratique moyenne (RMSE)
print(f"RMSE_Dep : {rmse}")

# Utilisez RegressionEvaluator pour évaluer le modèle pour l'arrivé
evaluator = RegressionEvaluator(labelCol="ArrDelay", predictionCol="prediction_Arri", metricName="rmse")
rmse = evaluator.evaluate(predictions_Arr)

# Affichez la racine carrée de l'erreur quadratique moyenne (RMSE)
print(f"RMSE_Arr : {rmse}")

#suprimer colones DayofMonth|DayOfWeek|Carrier du predictions_Dep

predictions_Dep = predictions_Dep.drop(*["DayofMonth","DayOfWeek","Carrier","Carrier_to_numérique","features"])

prediction = predictions_Dep.join(predictions_Arr.select("ArrDelay", "prediction_Arri"), on="ArrDelay", how="left")

#prediction.show()

prediction_Total = prediction.withColumn("prediction_total", col("prediction_Depp") + col("prediction_Arri"))

prediction_Total.show()

moyenne = prediction_Total.select(mean("prediction_total")).collect()[0][0]

# Affichage de la moyenne
print("le retard moyen est :", moyenne)

prediction_Total = prediction_Total.withColumn("retard", when( (col("prediction_total") < moyenne), 1).otherwise(0))

prediction_Total.show(truncate=False)
prediction_Total.printSchema()

#prediction_Total = prediction_Total.coalesce(1)

#prediction_Total.write.format("csv").option("header", "true").mode("overwrite").save("/home/gomes/Documents/cours/conceptiin pleippleain/examen/predection.csv")



############################################################################################################################



### Utilisation de la classification


from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.sql import SparkSession
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

from pyspark.ml.feature import StringIndexer
from pyspark.sql.functions import col, when

# 1. Créez une session Spark
spark = SparkSession.builder.appName("RandomForest").getOrCreate()
# 2. Chargez le fichier CSV dans un DataFrame
airports= spark.read.csv("airports.csv",header=True, inferSchema=True)

df_flight = spark.read.csv("flights.csv" , header=True, inferSchema=True)

df_raw_flight = spark.read.csv("raw-flight-data.csv" , header=True, inferSchema=True)

dat=df_raw_flight.intersect(df_flight)


sans_doublons=dat.dropDuplicates()

df=sans_doublons.na.drop()

df_filtre = df.filter((df["DepDelay"].cast("int").isNotNull()) & (df["DepDelay"] >= 0) | (df["DepDelay"] < 0 ) &
                      (df["ArrDelay"].cast("int").isNotNull()) & (df["ArrDelay"] >= 0) |  (df["ArrDelay"] < 0))
    
stringIndexer = StringIndexer(inputCol="Carrier", outputCol="Carrier_to_numérique")

# Ajustez le modèle de conversion aux données
model = stringIndexer.fit(df_filtre )

# Transformez les données pour ajouter la colonne numérique
data = model.transform(df_filtre )
data = data.drop("Carrier")
data.printSchema()


# ajout d'une colonne de retard binaire
data = data.withColumn("class_depdelay", when(col("DepDelay") < 0, 1.0).otherwise(0.0))
data = data.withColumn("class_arrdelay", when(col("ArrDelay") < 0, 1.0).otherwise(0.0))
input_cols = ["DayofMonth","DayOfWeek","OriginAirportID","DestAirportID","DepDelay","ArrDelay","Carrier_to_numérique","class_depdelay","class_arrdelay"]


# Créez le VectorAssembler
assembler = VectorAssembler(inputCols=input_cols, outputCol="features")

# Transformez le DataFrame en ajoutant la colonne "features"
assembled_df= assembler.transform(data)
#assembled_df.select('features').show(5)

# Utilisez la méthode randomSplit pour effectuer la division
train, test = assembled_df.randomSplit([0.8, 0.2])

# Random Forest for class_arrdelay
rf = RandomForestClassifier(featuresCol="features", labelCol="class_arrdelay", numTrees=10, predictionCol="prediction_arr")
rf_model = rf.fit(train)

classif = rf_model.transform(test)

# Evaluate the model
evaluator = MulticlassClassificationEvaluator(labelCol="class_arrdelay", predictionCol="prediction_arr", metricName="accuracy")
accuracy = evaluator.evaluate(classif)
print("Accuracy:", accuracy)
# afficher les resu
classif.select("features","class_arrdelay","prediction_arr").show()


 Random Forest for class_depdelay
rf = RandomForestClassifier(featuresCol="features", labelCol="class_depdelay", numTrees=10, predictionCol="prediction_dep")
rf_model = rf.fit(train)

classif = rf_model.transform(test)

 Evaluate the model
evaluator = MulticlassClassificationEvaluator(labelCol="class_depdelay", predictionCol="prediction_dep", metricName="accuracy")
accuracy = evaluator.evaluate(classif)
print("Accuracy:", accuracy)
# afficher les resu
classif.select("features","class_depdelay","prediction_dep").show()
spark.stop()



#############################################################################################################################


###manipulation Dags pour une seul tache

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import StringIndexer
from pyspark.sql.functions import col, when, mean

# Définissez la fonction contenant votre code
def run_spark_job():
    # 1. Créez une session Spark
    spark = SparkSession.builder.appName("LinearRegressionExample").getOrCreate()

    # 2. Chargez les fichiers CSV dans des DataFrames
    airports = spark.read.csv("/home/gomes/Documents/cours/conception pleine/examen/archive/airports.csv", header=True, inferSchema=True)
    df_flight = spark.read.csv("/home/gomes/Documents/cours/conception pleine/examen/archive/flights.csv", header=True, inferSchema=True)
    df_raw_flight = spark.read.csv("/home/gomes/Documents/cours/conception pleine/examen/archive/raw-flight-data.csv", header=True, inferSchema=True)

    dat = df_raw_flight.intersect(df_flight)
    sans_doublons = dat.dropDuplicates()
    df = sans_doublons.na.drop()

    df_filtre = df.filter(
        (df["DepDelay"].cast("int").isNotNull()) & ((df["DepDelay"] >= 0) | (df["DepDelay"] < 0)) &
        (df["ArrDelay"].cast("int").isNotNull()) & ((df["ArrDelay"] >= 0) | (df["ArrDelay"] < 0))
    )

    stringIndexer = StringIndexer(inputCol="Carrier", outputCol="Carrier_to_numérique")
    model = stringIndexer.fit(df_filtre)
    data = model.transform(df_filtre)

    input_cols = [col for col in df.columns if col not in ['Carrier']]
    assembler = VectorAssembler(inputCols=input_cols, outputCol="features")
    data = assembler.transform(data)

    train_ratio = 0.8
    test_ratio = 0.2
    train, test = data.randomSplit([train_ratio, test_ratio])

    lr = LinearRegression(featuresCol="features", labelCol="DepDelay")
    model = lr.fit(train)
    predictions_Dep = model.transform(test)
    predictions_Dep = predictions_Dep.withColumnRenamed("prediction", "prediction_Depp")

    lr_2 = LinearRegression(featuresCol="features", labelCol="ArrDelay")
    model = lr_2.fit(train)
    predictions_Arr = model.transform(test)
    predictions_Arr = predictions_Arr.withColumnRenamed("prediction", "prediction_Arri")

    evaluator = RegressionEvaluator(labelCol="DepDelay", predictionCol="prediction_Depp", metricName="rmse")
    rmse = evaluator.evaluate(predictions_Dep)

    evaluator = RegressionEvaluator(labelCol="ArrDelay", predictionCol="prediction_Arri", metricName="rmse")
    rmse = evaluator.evaluate(predictions_Arr)

    predictions_Dep = predictions_Dep.drop(*["DayofMonth", "DayOfWeek", "Carrier", "Carrier_to_numérique", "features"])
    prediction = predictions_Dep.join(predictions_Arr.select("ArrDelay", "prediction_Arri"), on="ArrDelay", how="left")
    prediction_Total = prediction.withColumn("prediction_total", col("prediction_Depp") + col("prediction_Arri"))
    moyenne = prediction_Total.select(mean("prediction_total")).collect()[0][0]
    prediction_Total = prediction_Total.withColumn("retard", when((col("prediction_total") < moyenne), 1).otherwise(0))
    prediction_Total = prediction_Total.coalesce(1)
    prediction_Total.write.format("csv").option("header", "true").mode("overwrite").save("/home/gomes/Documents/cours/conception pleine/examen/output.csv")

# Définissez les paramètres de votre DAG
default_args = {
    'owner': 'your_name',
    'start_date': datetime(2023, 1, 1),
    'schedule_interval': '@daily',  # Planification quotidienne
}

# Créez un objet DAG
dag = DAG('your_airflow_dag_id', default_args=default_args, catchup=False)

# Définissez une tâche qui exécute votre fonction
run_task = PythonOperator(
    task_id='run_spark_job',
    python_callable=run_spark_job,
    dag=dag,
)

if __name__ == "__main__":
    dag.cli()




##################################################################################################################

##### Manipulation dags pour 10 taches


from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import StringIndexer
from pyspark.sql.functions import col, when, mean

# Fonction contenant le code PySpark pour charger les données
def load_data():
    spark = SparkSession.builder.appName("LoadData").getOrCreate()
    airports = spark.read.csv("/home/gomes/Documents/cours/conceptiin pleippleain/examen/archive/airports.csv", header=True, inferSchema=True)
    df_flight = spark.read.csv("/home/gomes/Documents/cours/conceptiin pleippleain/examen/archive/flights.csv", header=True, inferSchema=True)
    df_raw_flight = spark.read.csv("/home/gomes/Documents/cours/conceptiin pleippleain/examen/archive/raw-flight-data.csv", header=True, inferSchema=True)
    
    dat = df_raw_flight.intersect(df_flight)
    sans_doublons = dat.dropDuplicates()
    df = sans_doublons.na.drop()

    df_filtre = df.filter((df["DepDelay"].cast("int").isNotNull()) & ((df["DepDelay"] >= 0) | (df["DepDelay"] < 0)) &
                          (df["ArrDelay"].cast("int").isNotNull()) & ((df["ArrDelay"] >= 0) | (df["ArrDelay"] < 0)))

# Fonction pour appliquer StringIndexer
def apply_string_indexer():
    stringIndexer = StringIndexer(inputCol="Carrier", outputCol="Carrier_to_numérique")
    model = stringIndexer.fit(df_filtre)
    data = model.transform(df_filtre)

# Fonction pour créer VectorAssembler
def create_vector_assembler():
    input_cols = [col for col in df.columns if col not in ['Carrier']]
    assembler = VectorAssembler(inputCols=input_cols, outputCol="features")
    data = assembler.transform(data)

# Fonction pour entraîner le modèle de régression linéaire pour le départ
def train_linear_regression_depart():
    train_ratio = 0.8
    test_ratio = 0.2
    train, test = data.randomSplit([train_ratio, test_ratio])
    lr = LinearRegression(featuresCol="features", labelCol="DepDelay")
    model = lr.fit(train)
    predictions_Dep = model.transform(test)
    predictions_Dep = predictions_Dep.withColumnRenamed("prediction", "prediction_Depp")

# Fonction pour entraîner le modèle de régression linéaire pour l'arrivée
def train_linear_regression_arrivee():
    lr_2 = LinearRegression(featuresCol="features", labelCol="ArrDelay")
    model = lr_2.fit(train)
    predictions_Arr = model.transform(test)
    predictions_Arr = predictions_Arr.withColumnRenamed("prediction", "prediction_Arri")

# Fonction pour évaluer le modèle pour le départ
def evaluate_linear_regression_depart():
    evaluator = RegressionEvaluator(labelCol="DepDelay", predictionCol="prediction_Depp", metricName="rmse")
    rmse = evaluator.evaluate(predictions_Dep)

# Fonction pour évaluer le modèle pour l'arrivée
def evaluate_linear_regression_arrivee():
    evaluator = RegressionEvaluator(labelCol="ArrDelay", predictionCol="prediction_Arri", metricName="rmse")
    rmse = evaluator.evaluate(predictions_Arr)

# Fonction pour supprimer les colonnes inutiles
def remove_columns():
    predictions_Dep = predictions_Dep.drop(*["DayofMonth", "DayOfWeek", "Carrier", "Carrier_to_numérique", "features"])

# Fonction pour calculer la moyenne des prédictions totales
def calculate_average():
    prediction = predictions_Dep.join(predictions_Arr.select("ArrDelay", "prediction_Arri"), on="ArrDelay", how="left")
    prediction_Total = prediction.withColumn("prediction_total", col("prediction_Depp") + col("prediction_Arri"))
    moyenne = prediction_Total.select(mean("prediction_total")).collect()[0][0]
    prediction_Total = prediction_Total.withColumn("retard", when((col("prediction_total") < moyenne), 1).otherwise(0))

# Paramètres par défaut du DAG
default_args = {
    'owner': 'your_name',
    'start_date': datetime(2023, 1, 1),
    'schedule_interval': '@daily',  # Planification quotidienne
}

# Créez un objet DAG
dag = DAG('10_Tâches', default_args=default_args, catchup=False)

# Définissez 10 tâches pour exécuter les étapes du code PySpark
tasks = [
    PythonOperator(task_id='load_data', python_callable=load_data, dag=dag),
    PythonOperator(task_id='apply_string_indexer', python_callable=apply_string_indexer, dag=dag),
    PythonOperator(task_id='create_vector_assembler', python_callable=create_vector_assembler, dag=dag),
    PythonOperator(task_id='train_linear_regression_depart', python_callable=train_linear_regression_depart, dag=dag),
    PythonOperator(task_id='train_linear_regression_arrivee', python_callable=train_linear_regression_arrivee, dag=dag),
    PythonOperator(task_id='evaluate_linear_regression_depart', python_callable=evaluate_linear_regression_depart, dag=dag),
    PythonOperator(task_id='evaluate_linear_regression_arrivee', python_callable=evaluate_linear_regression_arrivee, dag=dag),
    PythonOperator(task_id='remove_columns', python_callable=remove_columns, dag=dag),
    PythonOperator(task_id='calculate_average', python_callable=calculate_average, dag=dag),
]

# Définissez les dépendances entre les tâches
for i in range(1, len(tasks)):
    tasks[i] >> tasks[i-1]

if __name__ == "__main__":
    dag.cli()







